import os
import logging
from datetime import datetime
import argparse
import json
import time
import uuid
from pathlib import Path
from typing import Dict, List, Optional, Union, Set, Any
from dataclasses import dataclass

import torch
from fastapi import FastAPI, HTTPException, Depends
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, field_validator, ValidationInfo
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage
from green_bit_llm.langchain import GreenBitPipeline, ChatGreenBit
from transformers import AutoTokenizer
import asyncio
from concurrent.futures import ThreadPoolExecutor
from green_bit_llm.serve.auth import get_api_key_auth, APIKeyAuth
from starlette.middleware.base import BaseHTTPMiddleware
from dotenv import load_dotenv


thread_pool = ThreadPoolExecutor()

PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent
LOG_DIR = PROJECT_ROOT / "logs"

# Global configurations
server_config = None
model_provider = None
logger = None

# Global UE confidence scorers
UE_MODELS = {
    "qwen": "qwen2.5",
    "llama": "llama-3"
}
# Global confidence scorers
_confidence_scorers = {}


class RateLimitMiddleware(BaseHTTPMiddleware):
    """Middleware to handle concurrent request cleanup."""

    def __init__(self, app, auth_handler: APIKeyAuth):
        super().__init__(app)
        self.auth_handler = auth_handler

    async def dispatch(self, request, call_next):
        api_key = request.headers.get("X-Api-Key")

        try:
            response = await call_next(request)
            return response
        finally:
            if api_key:
                await self.auth_handler.rate_limiter.release_concurrent_request(api_key)


@dataclass
class StoppingCriteria:
    stop_met: bool
    reason: str


def stopping_criteria(tokens: List[int], stop_sequences: List[List[int]], eos_token_id: int) -> StoppingCriteria:
    """Check if stopping criteria are met."""
    if not tokens:
        return StoppingCriteria(False, "")

    # Check for EOS token
    if tokens[-1] == eos_token_id:
        return StoppingCriteria(True, "eos_token")

    # Check for stop sequences
    for stop_seq in stop_sequences:
        if len(tokens) >= len(stop_seq):
            if tokens[-len(stop_seq):] == stop_seq:
                return StoppingCriteria(True, "stop_sequence")

    return StoppingCriteria(False, "")


def sequence_overlap(tokens: List[int], stop_sequence: List[int]) -> bool:
    """Check if the tokens end with a partial stop sequence."""
    if not tokens or not stop_sequence:
        return False

    for i in range(1, len(stop_sequence)):
        if tokens[-i:] == stop_sequence[:i]:
            return True
    return False


def convert_chat(messages: List[Dict[str, str]]) -> str:
    """Convert chat messages to a single string format."""
    formatted_messages = []
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        if role == "system":
            formatted_messages.append(f"System: {content}")
        elif role == "user":
            formatted_messages.append(f"User: {content}")
        elif role == "assistant":
            formatted_messages.append(f"Assistant: {content}")
    return "\n".join(formatted_messages) + "\nAssistant:"


def convert_model_name_to_url_path(model_name: str) -> str:
    """
    Convert a model name to a URL-safe path segment.
    Examples:
        "GreenBitAI/Llama-3-8B-instruct-layer-mix-bpw-4.0-mlx" ->
        "GreenBitAI-Llama-3-8B-instruct-layer-mix-bpw-4.0-mlx"
    """
    # Replace forward slashes with dashes
    url_safe_name = model_name.replace("/", "-")

    # Remove any special characters that might cause issues in URLs
    # Keep alphanumeric characters, dashes, and underscores
    url_safe_name = "".join(c for c in url_safe_name
                            if c.isalnum() or c in "-_")

    # Remove any repeated dashes
    while "--" in url_safe_name:
        url_safe_name = url_safe_name.replace("--", "-")

    # Remove leading or trailing dashes
    url_safe_name = url_safe_name.strip("-")

    return url_safe_name


def get_model_endpoint_path(model_name: str, endpoint_type: str) -> str:
    """Generate the full API endpoint path for a given model and endpoint type."""
    safe_name = convert_model_name_to_url_path(model_name)
    return f"/v1/{safe_name}/{endpoint_type}"


def setup_logging():
    """Configure logging for the FastAPI server."""
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = LOG_DIR / f"server_{timestamp}.log"

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )

    logger = logging.getLogger("greenbit_server")
    logger.info(f"Starting GreenBit API server. Log file: {log_file}")
    return logger


def parse_args():
    parser = argparse.ArgumentParser(
        description="GreenBit FastAPI Server.",
        formatter_class = argparse.RawDescriptionHelpFormatter,
        epilog="""
            Example usage:
               # method 1ï¼šread api key from request header
                curl -X POST "http://localhost:8000/v1/model-name/completions" \
                     -H "X-Api-Key: your-api-key" \
                     -H "Content-Type: application/json" \
                     -d '{"prompt": "Hello", "max_tokens": 100}'
                
                # Method 2: Use the API key in the environment variable (no need to specify it in the request header)
                curl -X POST "http://localhost:8000/v1/model-name/completions" \
                     -H "Content-Type: application/json" \
                     -d '{"prompt": "Hello", "max_tokens": 100}'
            """
    )
    # Server configuration
    parser.add_argument("--host", type=str, default="127.0.0.1", help="Host address to bind the server")
    parser.add_argument("--port", type=int, default=8000, help="Port number to run the server")

    # Model configuration - support both single and multiple models
    model_group = parser.add_mutually_exclusive_group()
    model_group.add_argument("--model", type=str, help="Path to the model weights, tokenizer, and config")
    model_group.add_argument("--model_list", type=str, nargs="+", help="List of model paths to serve")

    # Additional configurations
    parser.add_argument("--device", type=str, default="cuda:0", help="Device to run the model on")
    parser.add_argument("--max_new_tokens", type=int, default=100, help="Maximum number of new tokens to generate")
    parser.add_argument("--temperature", type=float, default=0.7, help="Sampling temperature")
    parser.add_argument("--use_default_chat_template", action="store_true",
                        help="Use the default chat template from the model's tokenizer")
    parser.add_argument("--chat_template", type=str, default="",
                        help="Custom chat template to use instead of default")
    parser.add_argument("--eos_token", type=str, default="<|eot_id|>",
                        help="End of sequence token for tokenizer")
    parser.add_argument("--db_file_path", type=str, default="db/greenbit.db",
                        help="Path to the uncertainty estimation parameters database")
    parser.add_argument("--trust_remote_code", action="store_true",
                        help="Allow loading remote code in tokenizer")
    parser.add_argument("--seqlen", type=int, default=2048, help="Maximum sequence length")
    parser.add_argument("--device_map", type=str, default="auto", help="Device mapping strategy")
    parser.add_argument("--env_file", type=str, default=".env", help="Path to .env file")

    return parser.parse_args()


class ServerConfig:
    def __init__(self, host: str = "127.0.0.1", port: int = 8000, **kwargs):
        self.host = host
        self.port = port
        self.model_config = argparse.Namespace(**kwargs)

        # Store the list of models to serve
        self.models_to_serve: Set[str] = set()
        if kwargs.get("model_list"):
            self.models_to_serve.update(kwargs["model_list"])
        elif kwargs.get("model"):
            self.models_to_serve.add(kwargs["model"])


class ModelProvider:
    def __init__(self, cli_args: argparse.Namespace):
        self.cli_args = cli_args
        self.model_cache = {}  # Cache for loaded models

        try:
            # Load all specified models
            if self.cli_args.model_list:
                for model_path in self.cli_args.model_list:
                    self.load_model(model_path)
            elif self.cli_args.model:
                self.load_model(self.cli_args.model)

        except Exception as e:
            logger.error(f"Failed to initialize ModelProvider: {str(e)}")
            raise

    def load_model(self, model_path: str):
        """Load a model and its tokenizer, with caching."""
        if model_path in self.model_cache:
            return self.model_cache[model_path]

        try:
            # Initialize tokenizer with custom configuration
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=self.cli_args.trust_remote_code,
                eos_token="<|im_end|>" if model_path.lower().__contains__("qwen") else "<|eot_id|>",
            )

            # Configure chat template
            if self.cli_args.chat_template:
                tokenizer.chat_template = self.cli_args.chat_template
            elif self.cli_args.use_default_chat_template:
                if not tokenizer.chat_template:
                    tokenizer.chat_template = tokenizer.default_chat_template

            # Initialize pipeline
            pipeline = GreenBitPipeline.from_model_id(
                model_id=model_path,
                device=self.cli_args.device,
                tokenizer=tokenizer,
                model_kwargs={
                    "dtype": torch.half,
                    "device_map": self.cli_args.device_map,
                    "seqlen": self.cli_args.seqlen,
                    "requires_grad": False
                },
                pipeline_kwargs={
                    "max_new_tokens": self.cli_args.max_new_tokens,
                    "temperature": self.cli_args.temperature,
                    "eos_token_id": tokenizer.eos_token_id
                },
            )

            chat_model = ChatGreenBit(llm=pipeline)

            # Cache the loaded model components
            self.model_cache[model_path] = {
                'chat_model': chat_model,
                'tokenizer': tokenizer,
                'pipeline': pipeline
            }

            logger.info(f"Successfully loaded model {model_path}")
            return self.model_cache[model_path]

        except Exception as e:
            logger.error(f"Failed to load model {model_path}: {str(e)}")
            raise

    def get_model(self, model_path: str):
        """Get a loaded model by its path."""
        if model_path not in self.model_cache:
            return self.load_model(model_path)
        return self.model_cache[model_path]


class CompletionRequest(BaseModel):
    """
    Example requests:
    # Single completion
    request = CompletionRequest(
        model="model_name",
        prompt="Hello world"
    )
    # Batch completion
    request = CompletionRequest(
        model="model_name",
        prompt=["Hello world", "How are you"]
    )
    """
    model: str
    prompt: Union[str, List[str]]
    max_tokens: int = 100
    temperature: float = 1.0
    top_p: float = 1.0
    stream: bool = False
    stop: Optional[Union[str, List[str]]] = None
    logit_bias: Optional[Dict[str, float]] = None
    repetition_penalty: float = 1.0
    repetition_context_size: int = 20
    with_hidden_states: bool = False
    remote_score: bool = True
    api_key: str = ""

    @field_validator('prompt')
    @classmethod
    def validate_prompt(cls, v):
        if isinstance(v, list):
            if not v:
                raise ValueError("Prompt list cannot be empty")
            if not all(isinstance(p, str) for p in v):
                raise ValueError("All prompts must be strings")
            if any(not p.strip() for p in v):
                raise ValueError("Prompts cannot be empty strings")
        elif not v.strip():
            raise ValueError("Prompt cannot be empty")
        return v

    @field_validator('stream', 'with_hidden_states')
    @classmethod
    def validate_stream_with_batch(cls, v, info: ValidationInfo):
        prompt = info.data.get('prompt')
        if v and isinstance(prompt, list) and len(prompt) > 1:
            raise ValueError("Streaming is not supported with batch requests")
        return v


class ChatCompletionRequest(BaseModel):
    """
    Example requests:
    # Single chat
    request = ChatCompletionRequest(
        model="model_name",
        messages=[
            {"role": "user", "content": "Hello"}
        ]
    )
    # Batch chat
    request = ChatCompletionRequest(
        model="model_name",
        messages=[
            [{"role": "user", "content": "Hello"}],
            [{"role": "user", "content": "Hi"}]
        ]
    )
    """
    model: str
    messages: Union[List[Dict[str, str]], List[List[Dict[str, str]]]]
    max_tokens: int = 100
    temperature: float = 1.0
    top_p: float = 1.0
    stream: bool = False
    stop: Optional[Union[str, List[str]]] = None
    logit_bias: Optional[Dict[str, float]] = None
    repetition_penalty: float = 1.0
    repetition_context_size: int = 20
    with_hidden_states: bool = False
    remote_score: bool = True
    api_key: str = ""

    @field_validator('messages')
    @classmethod
    def validate_messages(cls, v):
        if not v:
            raise ValueError("Messages cannot be empty")

        if isinstance(v[0], dict):
            cls._validate_single_conversation(v)
        elif isinstance(v[0], list):
            if not all(isinstance(conv, list) for conv in v):
                raise ValueError("All items in messages must be lists of messages")
            for conv in v:
                cls._validate_single_conversation(conv)
        else:
            raise ValueError("Invalid messages format")
        return v

    @staticmethod
    def _validate_single_conversation(messages):
        if not messages:
            raise ValueError("Conversation cannot be empty")

        for msg in messages:
            if not isinstance(msg, dict):
                raise ValueError("Each message must be a dictionary")

            if 'role' not in msg or 'content' not in msg:
                raise ValueError("Each message must have 'role' and 'content'")

            if msg['role'] not in ['system', 'user', 'assistant']:
                raise ValueError("Message role must be 'system', 'user', or 'assistant'")

            if not isinstance(msg['content'], str) or not msg['content'].strip():
                raise ValueError("Message content must be a non-empty string")

    @field_validator('stream', 'with_hidden_states')
    @classmethod
    def validate_stream_with_batch(cls, v, info: ValidationInfo):
        messages = info.data.get('messages', [])
        is_batch = isinstance(messages, list) and messages and isinstance(messages[0], list)
        if v and is_batch and len(messages) > 1:
            raise ValueError("Streaming is not supported with batch requests")
        return v


async def stream_completion(request: CompletionRequest, chat_model: ChatGreenBit):
    created = int(time.time())
    request_id = f"cmpl-{uuid.uuid4()}"

    try:
        # Prepare and wrap generation parameters
        generation_kwargs = {
            "temperature": request.temperature,
            "max_new_tokens": request.max_tokens
        }
        wrapped_kwargs = {
            "pipeline_kwargs": generation_kwargs
        }

        # Convert string prompt to HumanMessage
        messages = [HumanMessage(content=request.prompt)]

        # Get tokenizer for counting tokens
        tokenizer = chat_model.llm.pipeline.tokenizer

        # Count input tokens
        input_tokens = len(tokenizer.encode(request.prompt))
        output_tokens = 0
        is_first_chunk = True

        for chunk in chat_model.stream(messages, **wrapped_kwargs):
            # Count tokens in this chunk
            chunk_tokens = len(tokenizer.encode(chunk.message.content))
            output_tokens += chunk_tokens

            response = {
                "id": request_id,
                "object": "text_completion",
                "created": created,
                "model": request.model,
                "choices": [
                    {
                        "text": chunk.message.content,
                        "index": 0,
                        "logprobs": None,
                        "finish_reason": None,
                    }
                ]
            }

            # Add usage info in first chunk
            if is_first_chunk:
                response["usage"] = {
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens
                }
                is_first_chunk = False

            yield f"data: {json.dumps(response)}\n\n"

        # Final chunk with complete usage stats
        response = {
            "id": request_id,
            "object": "text_completion",
            "created": created,
            "model": request.model,
            "choices": [
                {
                    "text": "",
                    "index": 0,
                    "logprobs": None,
                    "finish_reason": "max_tokens" if len(output_tokens) == request.max_tokens else "stop",
                }
            ],
            "usage": {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            }
        }
        yield f"data: {json.dumps(response)}\n\n"
        yield "data: [DONE]\n\n"
    except Exception as e:
        logger.error(f"Streaming completion failed: {str(e)}")
        raise


async def stream_chat_completion(request: ChatCompletionRequest, chat_model: ChatGreenBit,
                               messages_list: List[List[BaseMessage]]):
    created = int(time.time())
    request_id = f"chatcmpl-{uuid.uuid4()}"

    try:
        # Prepare and wrap generation parameters
        generation_kwargs = {
            "temperature": request.temperature,
            "max_new_tokens": request.max_tokens
        }
        wrapped_kwargs = {
            "pipeline_kwargs": generation_kwargs
        }

        # Get tokenizer for counting tokens
        tokenizer = chat_model.llm.pipeline.tokenizer

        # Convert messages to prompt and count input tokens
        prompt = await asyncio.get_event_loop().run_in_executor(
            thread_pool,
            chat_model._to_chat_prompt,
            messages_list[0]
        )
        input_tokens = len(tokenizer.encode(prompt))
        output_tokens = 0
        is_first_chunk = True

        for chunk in chat_model.stream(messages_list[0], **wrapped_kwargs):
            # Count tokens in this chunk
            chunk_tokens = len(tokenizer.encode(chunk.message.content))
            output_tokens += chunk_tokens

            response = {
                "id": request_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": request.model,
                "choices": [
                    {
                        "delta": {
                            "role": "assistant",
                            "content": chunk.message.content
                        },
                        "index": 0,
                        "finish_reason": None,
                    }
                ]
            }
            # Add usage info in first chunk
            if is_first_chunk:
                response["usage"] = {
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": input_tokens + output_tokens
                }
                is_first_chunk = False

            yield f"data: {json.dumps(response)}\n\n"

        # Final chunk with complete usage stats
        response = {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": request.model,
            "choices": [
                {
                    "delta": {
                        "role": "assistant",
                        "content": ""
                    },
                    "index": 0,
                    "finish_reason": "max_tokens" if len(output_tokens) == request.max_tokens else "stop",
                }
            ],
            "usage": {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens
            }
        }
        yield f"data: {json.dumps(response)}\n\n"
        yield "data: [DONE]\n\n"
    except Exception as e:
        logger.error(f"Streaming chat completion failed: {str(e)}")
        raise


async def generate_completion(request: CompletionRequest, chat_model: ChatGreenBit):
    created = int(time.time())
    request_id = f"cmpl-{uuid.uuid4()}"

    try:
        generation_kwargs = {
            "temperature": request.temperature,
            "max_tokens": request.max_tokens
        }

        # Wrap generation parameters
        wrapped_kwargs = {
            "pipeline_kwargs": generation_kwargs,
            "with_hidden_states": request.with_hidden_states
        }

        prompts = request.prompt if isinstance(request.prompt, list) else [request.prompt]

        # thread pool
        llm_result = await asyncio.get_event_loop().run_in_executor(
            thread_pool,
            lambda: chat_model.llm._generate(
                prompts=prompts,
                **wrapped_kwargs
            )
        )

        # handle results
        choices = []
        for idx, generations in enumerate(llm_result.generations):
            gen_info = generations[0].generation_info or {}
            hidden_states = gen_info.get("hidden_states")
            score = None

            if request.with_hidden_states and hidden_states is not None:
                if request.remote_score:
                    model_key = "qwen" if request.model.lower().__contains__("qwen") else "llama"
                    scorer = _confidence_scorers.get(model_key)
                    if scorer:
                        # thread pool for scoring
                        scores = await asyncio.get_event_loop().run_in_executor(
                            thread_pool,
                            scorer.calculate_confidence,
                            hidden_states
                        )
                        score = scores if isinstance(scores, list) else [scores]
                    hidden_states=None
                else:
                    hidden_states = await asyncio.get_event_loop().run_in_executor(
                        thread_pool,
                        lambda: hidden_states.cpu().tolist()
                    )

            # Let tokenizer determine if max_tokens was reached
            tokenizer = chat_model.llm.pipeline.tokenizer
            generated_text = generations[0].text
            finish_reason = "length" if len(tokenizer.encode(generated_text)) >= request.max_tokens else "stop"

            choices.append({
                "text": generations[0].text,
                "index": idx,
                "logprobs": None,
                "finish_reason": finish_reason,
                "hidden_states": hidden_states,
                "confidence_score": score[idx] if score else None
            })

        async def calculate_tokens():
            try:
                tokenizer = chat_model.llm.pipeline.tokenizer
                # thread pool
                prompt_tokens = await asyncio.gather(*[
                    asyncio.get_event_loop().run_in_executor(
                        thread_pool,
                        lambda p=p: len(tokenizer.encode(p))
                    )
                    for p in prompts
                ])
                completion_tokens = await asyncio.gather(*[
                    asyncio.get_event_loop().run_in_executor(
                        thread_pool,
                        lambda c=c: len(tokenizer.encode(c["text"]))
                    )
                    for c in choices
                ])
                return {
                    "input_tokens": sum(prompt_tokens),
                    "output_tokens": sum(completion_tokens),
                    "total_tokens": sum(prompt_tokens) + sum(completion_tokens)
                }
            except Exception as e:
                logger.warning(f"Error calculating exact token counts: {str(e)}. Using approximate count.")
                prompt_tokens = sum(len(p.split()) for p in prompts)
                completion_tokens = sum(len(c["text"].split()) for c in choices)
                return {
                    "input_tokens": prompt_tokens,
                    "output_tokens": completion_tokens,
                    "total_tokens": prompt_tokens + completion_tokens
                }

        usage = await calculate_tokens()

        return {
            "id": request_id,
            "object": "text_completion",
            "created": created,
            "model": request.model,
            "choices": choices,
            "usage": usage
        }
    except Exception as e:
        logger.error(f"Completion generation failed: {str(e)}")
        if isinstance(e, torch.cuda.OutOfMemoryError):
            raise HTTPException(status_code=503,
                              detail="GPU memory exhausted, please try with shorter input or smaller batch")
        elif isinstance(e, ValueError):
            raise HTTPException(status_code=400, detail=str(e))
        raise HTTPException(status_code=500, detail=str(e))


async def generate_chat_completion(request: ChatCompletionRequest, chat_model: ChatGreenBit,
                                 messages_list: List[List[HumanMessage]]):
    created = int(time.time())
    request_id = f"chatcmpl-{uuid.uuid4()}"

    try:
        generation_kwargs = {
            "temperature": request.temperature,
            "top_p": request.top_p,
            "max_tokens": request.max_tokens
        }

        # Wrap generation parameters
        wrapped_kwargs = {
            "pipeline_kwargs": generation_kwargs,
            "with_hidden_states": request.with_hidden_states
        }

        # thread pool
        prompts = await asyncio.gather(*[
            asyncio.get_event_loop().run_in_executor(
                thread_pool,
                chat_model._to_chat_prompt,
                msgs
            )
            for msgs in messages_list
        ])

        # thread pool
        llm_result = await asyncio.get_event_loop().run_in_executor(
            thread_pool,
            lambda: chat_model.llm._generate(
                prompts=prompts,
                **wrapped_kwargs
            )
        )

        # handl results
        choices = []
        for idx, generations in enumerate(llm_result.generations):
            gen_info = generations[0].generation_info or {}
            hidden_states = gen_info.get("hidden_states")
            score = None

            if request.with_hidden_states and hidden_states is not None:
                if request.remote_score:
                    model_key = "qwen" if request.model.lower().__contains__("qwen") else "llama"
                    scorer = _confidence_scorers.get(model_key)
                    if scorer:
                        # thread pool
                        scores = await asyncio.get_event_loop().run_in_executor(
                            thread_pool,
                            scorer.calculate_confidence,
                            hidden_states
                        )
                        score = scores if isinstance(scores, list) else [scores]

                    hidden_states = None
                else:
                    # CPU ops in thread pool
                    hidden_states = await asyncio.get_event_loop().run_in_executor(
                        thread_pool,
                        lambda: hidden_states.cpu().tolist()
                    )

            # Let tokenizer determine if max_tokens was reached
            tokenizer = chat_model.llm.pipeline.tokenizer
            generated_text = generations[0].text
            finish_reason = "length" if len(tokenizer.encode(generated_text)) >= request.max_tokens else "stop"

            choices.append({
                "message": {
                    "role": "assistant",
                    "content": generations[0].text,
                    "hidden_states": hidden_states,
                    "confidence_score": score[idx] if score else None
                },
                "index": idx,
                "finish_reason": finish_reason
            })

        async def calculate_tokens():
            try:
                tokenizer = chat_model.llm.pipeline.tokenizer

                prompt_tokens = await asyncio.gather(*[
                    asyncio.get_event_loop().run_in_executor(
                        thread_pool,
                        lambda p=p: len(tokenizer.encode(p))
                    )
                    for p in prompts
                ])
                completion_tokens = await asyncio.gather(*[
                    asyncio.get_event_loop().run_in_executor(
                        thread_pool,
                        lambda c=c: len(tokenizer.encode(c["message"]["content"]))
                    )
                    for c in choices
                ])
                return {
                    "input_tokens": sum(prompt_tokens),
                    "output_tokens": sum(completion_tokens),
                    "total_tokens": sum(prompt_tokens) + sum(completion_tokens)
                }
            except Exception as e:
                logger.warning(f"Error calculating exact token counts: {str(e)}. Using approximate count.")
                # fall back
                prompt_tokens = sum(sum(len(msg.content.split()) for msg in msgs) for msgs in messages_list)
                completion_tokens = sum(len(c["message"]["content"].split()) for c in choices)
                return {
                    "input_tokens": prompt_tokens,
                    "output_tokens": completion_tokens,
                    "total_tokens": prompt_tokens + completion_tokens
                }

        usage = await calculate_tokens()

        return {
            "id": request_id,
            "object": "chat.completion",
            "created": created,
            "model": request.model,
            "choices": choices,
            "usage": usage
        }
    except Exception as e:
        logger.error(f"Chat completion generation failed: {str(e)}")
        if isinstance(e, torch.cuda.OutOfMemoryError):
            raise HTTPException(status_code=503,
                              detail="GPU memory exhausted, please try with shorter input or smaller batch")
        elif isinstance(e, ValueError):
            raise HTTPException(status_code=400, detail=str(e))
        raise HTTPException(status_code=500, detail=str(e))


def create_app(args):
    """Create and configure the FastAPI application with routes."""

    # Initialize logging
    global logger
    logger = setup_logging()

    # CUDA specific optimization
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True

    # Initialize confidence scorers
    try:
        from green_bit_llm.routing import ConfidenceScorer
        for model_family, model_id in UE_MODELS.items():
            scorer = ConfidenceScorer(
                parameters_path=args.db_file_path,
                model_id=model_id
            )
            _confidence_scorers[model_family] = scorer
    except Exception as e:
        logger.error(f"Error loading confidence scorers: {str(e)}")

    app = FastAPI(
        title="GreenBit API",
        description="API using GreenBit models",
    )
    # Load environment variables
    if args.env_file and Path(args.env_file).exists():
        load_dotenv(args.env_file)

    # Update DB path from environment if provided
    db_path = os.getenv("LIBRA_DB_PATH", args.db_file_path)

    # Initialize auth handler
    auth_handler = APIKeyAuth(db_path)
    app.add_middleware(RateLimitMiddleware, auth_handler=auth_handler)

    # Add logging middleware
    @app.middleware("http")
    async def log_requests(request, call_next):
        start_time = time.time()
        try:
            response = await call_next(request)
            duration = time.time() - start_time
            logger.info(
                f"Request: {request.method} {request.url.path} "
                f"Status: {response.status_code} "
                f"Duration: {duration:.2f}s"
            )
            return response
        except Exception as e:
            logger.error(f"Request failed: {str(e)}")
            raise

    # Create server config with all arguments
    server_config = ServerConfig(
        host=args.host,
        port=args.port,
        model=args.model,
        model_list=args.model_list,
        device=args.device,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        use_default_chat_template=args.use_default_chat_template,
        chat_template=args.chat_template,
        eos_token=args.eos_token,
        trust_remote_code=args.trust_remote_code,
        env_file=args.env_file,
        db_file_path=args.db_file_path,
        seqlen=args.seqlen,
        device_map=args.device_map
    )

    # Initialize model provider
    model_provider = ModelProvider(server_config.model_config)

    # Helper function to create endpoints for a specific model
    def create_model_endpoints(model_path: str):
        completion_path = get_model_endpoint_path(model_path, "completions")
        chat_completion_path = get_model_endpoint_path(model_path, "chat/completions")

        logger.info(f"Creating endpoints for model {model_path}:")
        logger.info(f"  - Completion endpoint: {completion_path}")
        logger.info(f"  - Chat completion endpoint: {chat_completion_path}")

        @app.post(completion_path)
        async def create_completion(
            request: CompletionRequest,
            user_info: dict = Depends(get_api_key_auth)
        ):
            try:
                # Estimate total tokens
                if isinstance(request.prompt, list):
                    estimated_tokens = sum(len(p.split()) for p in request.prompt) + request.max_tokens * len(
                        request.prompt)
                else:
                    estimated_tokens = len(request.prompt.split()) + request.max_tokens

                # Check permissions
                auth_handler.check_permissions(user_info, "completion")

                # Check token limit
                auth_handler.check_token_limit(user_info, request.max_tokens)

                # Check rate limits with token estimate
                await auth_handler.check_rate_limits(
                    request.api_key,
                    user_info,
                    estimated_tokens
                )

                model_components = model_provider.get_model(model_path)
                chat_model = model_components['chat_model']

                if request.stream:
                    return StreamingResponse(
                        stream_completion(request, chat_model),
                        media_type="text/event-stream"
                    )
                else:
                    result = await generate_completion(request, chat_model)
                    return JSONResponse(result)
            except Exception as e:
                logger.error(f"Completion request failed for {model_path}: {str(e)}")
                raise HTTPException(status_code=500, detail=str(e))

        @app.post(chat_completion_path)
        async def create_chat_completion(
            request: ChatCompletionRequest,
            user_info: dict = Depends(get_api_key_auth)
        ):
            try:
                # Check permissions
                auth_handler.check_permissions(user_info, "chat")

                # Check token limit
                auth_handler.check_token_limit(user_info, request.max_tokens)

                # Rough token estimation for chat
                estimated_tokens = sum(
                    len(msg["content"].split())
                    for msg in (request.messages if isinstance(request.messages[0], dict)
                                else [item for sublist in request.messages for item in sublist])
                ) + request.max_tokens

                # Check rate limits with token estimate
                await auth_handler.check_rate_limits(
                    request.api_key,
                    user_info,
                    estimated_tokens
                )

                model_components = model_provider.get_model(model_path)
                chat_model = model_components['chat_model']

                if isinstance(request.messages[0], dict):  # single chat
                    messages_list = [[HumanMessage(content=msg["content"])
                                      if msg["role"] == "user"
                                      else SystemMessage(content=msg["content"])
                                      for msg in request.messages]]
                else:  # batch
                    messages_list = [[HumanMessage(content=msg["content"])
                                      if msg["role"] == "user"
                                      else SystemMessage(content=msg["content"])
                                      for msg in conversation]
                                     for conversation in request.messages]

                if request.stream:
                    return StreamingResponse(
                        stream_chat_completion(request, chat_model, messages_list),
                        media_type="text/event-stream"
                    )
                else:
                    result = await generate_chat_completion(request, chat_model, messages_list)
                    return JSONResponse(result)
            except Exception as e:
                logger.error(f"Chat completion request failed for {model_path}: {str(e)}")
                raise HTTPException(status_code=500, detail=str(e))

    # Create endpoints for each model
    for model_path in server_config.models_to_serve:
        create_model_endpoints(model_path)

    # Add root endpoint for API information
    @app.get("/")
    async def root():
        try:
            models = list(server_config.models_to_serve)
            endpoints = []

            for model in models:
                endpoints.extend([
                    get_model_endpoint_path(model, "completions"),
                    get_model_endpoint_path(model, "chat/completions")
                ])

            return {
                "api": "GreenBit API",
                "version": "1.0",
                "models": models,
                "endpoints": endpoints
            }
        except Exception as e:
            logger.error(f"Root endpoint request failed: {str(e)}")
            raise HTTPException(status_code=500, detail="Internal server error")

    @app.get("/health")
    async def health_check():
        try:
            # check CUDA available
            cuda_available = torch.cuda.is_available()
            if not cuda_available:
                raise HTTPException(status_code=503, detail="CUDA not available")
            return {
                "status": "healthy",
                "cuda_available": True
            }
        except Exception as e:
            raise HTTPException(status_code=503, detail=str(e))

    @app.exception_handler(Exception)
    async def global_exception_handler(request, exc):
        logger.error(f"Unhandled exception: {str(exc)}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={"message": "Internal server error"}
        )

    return app, server_config, logger

def main():
    try:
        import uvicorn
        args = parse_args()
        app, server_config, logger = create_app(args)
        logger.info(f"Starting server on {server_config.host}:{server_config.port}")
        uvicorn.run(app, host=server_config.host, port=server_config.port)
    except Exception as e:
        logger.error(f"Server startup failed: {str(e)}")
        raise

if __name__ == "__main__":
    main()
